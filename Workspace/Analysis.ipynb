{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../External_Functions')\n",
    "from ExternalFunctions import Chi2Regression, BinnedLH, UnbinnedLH, simpson38\n",
    "from ExternalFunctions import nice_string_output, add_text_to_ax \n",
    "\n",
    "from MplSetup import getColour, setMplParam\n",
    "\n",
    "# from AccessibilityUtils import onCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setMplParam(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNFLPlay():\n",
    "    subDirPath = '../NFLPlay/'\n",
    "    plays = pd.read_csv(subDirPath+'plays.csv')\n",
    "    return plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFLplays_raw = readNFLPlay()\n",
    "# 50 sec on Cyan's pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays_raw[NFLplays_raw['gameId']==26909][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays_raw[NFLplays_raw['gameId']==26910][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870384, 44)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFLplays_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in NFLplays_raw['formation'].unique():\n",
    "#     n = NFLplays_raw[NFLplays_raw['formation']== i ].shape[0]\n",
    "#     print(f'{i:10}----------{n:6d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ['playId', 'gameId']\n",
    "\n",
    "playCircumstance = ['playSequence', \n",
    "                'quarter', \n",
    "                'possessionTeamId',\n",
    "                'nonpossessionTeamId', \n",
    "                'playNumberByTeam',\n",
    "                'gameClock', \n",
    "                'down', \n",
    "                'distance',\n",
    "                'distanceToGoalPre',\n",
    "                'netYards',\n",
    "                'scorePossession',\n",
    "                'scoreNonpossession',\n",
    "                'fieldGoalProbability',]\n",
    "\n",
    "# classification\n",
    "playType = ['playType'\n",
    "            'huddle',\n",
    "            'formation']\n",
    "\n",
    "playResult = ['playType2', # only second item\n",
    "                'gameClockSecondsExpired',\n",
    "              'gameClockStoppedAfterPlay', \n",
    "               'noPlay', # is the play a penalty\n",
    "               'offensiveYards']\n",
    "\n",
    "playSubsequence = ['isClockRunning', \n",
    "                        'changePossession', \n",
    "                        'turnover',\n",
    "                        'safety',\n",
    "                        'firstDown',]\n",
    "\n",
    "idk = [ 'typeOfPlay',\n",
    "        'fourthDownConversion',\n",
    "        'thirdDownConversion',\n",
    "        'homeScorePre', \n",
    "        'visitingScorePre',\n",
    "        'homeScorePost',\n",
    "        'visitingScorePost',\n",
    "        'distanceToGoalPost']\n",
    "\n",
    "# the original dataset has 3 columns of their own prediction of the play we may be able to use them as a reference\n",
    "reference = ['evPre',\n",
    "             'evPost', \n",
    "             'evPlay',]\n",
    "\n",
    "exclude = [ 'playTypeDetailed', # redundant to playType2\n",
    "            'fieldPosition', \n",
    "            'playDescription',\n",
    "            'playStats',\n",
    "            'playDescriptionFull', \n",
    "            'efficientPlay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Algorithm\n",
    "1. playCircumstance -> XGB -> playType\n",
    "2. playCircumstance & playType -> Regression (NN?XGB?) -> playResult\n",
    "3. playResult -> **manual function** -> playCircumstance\n",
    "<!-- * updateCircumstance -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocess import printColumnsHasNan, printNonNumericColumns, runPreprocess, getStringValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* see the set of `'playType'` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays_raw['playType'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* see the lines that have NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printColumnsHasNan(NFLplays_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* see the lines that have non-numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printNonNumericColumns(NFLplays_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">runPreprocess</span> : this is where the collective preprocessing algorithms come into play!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFLplays = runPreprocess(NFLplays_raw, exclude, idk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870384"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFLplays.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* see the set of `'playType'` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays['playType'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `fieldGoalProbability` has nan for these `playType` values\n",
    "  * 'kickoff'\n",
    "  * 'xp'\n",
    "  * 'two-point'\n",
    "  * 'aborted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays[NFLplays['fieldGoalProbability'].isnull() & \n",
    "#                        (NFLplays['playType'] != 'kickoff') & \n",
    "#                        (NFLplays['playType'] != 'xp') &\n",
    "#                        (NFLplays['playType'] != 'two-point')&\n",
    "#                        (NFLplays['playType'] != 'aborted')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `huddle` has nan for these `playType` values\n",
    "  * 'kickoff'\n",
    "  * 'field goal'\n",
    "  * 'punt'\n",
    "  * 'xp'\n",
    "  * 'two-point'\n",
    "  * 'penalty'\n",
    "  * 'aborted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays[NFLplays['huddle'].isnull() & \n",
    "#                        (NFLplays['playType'] != 'kickoff') & \n",
    "#                        (NFLplays['playType'] != 'field goal') &\n",
    "#                         (NFLplays['playType'] != 'punt') &\n",
    "#                        (NFLplays['playType'] != 'xp')&\n",
    "#                           (NFLplays['playType'] != 'two-point')&\n",
    "#                           (NFLplays['playType'] != 'penalty')&\n",
    "#                        (NFLplays['playType'] != 'aborted')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays['playType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFLplays['gameClock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(getStringValue('playType', 0))\n",
    "# print(getStringValue('playType', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCircumstance(df):\n",
    "    return df[playCircumstance]\n",
    "def getPlayType(df):\n",
    "    return df[playType]\n",
    "def getPlayResult(df):\n",
    "    return df[playResult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBClassifierCore(X_train, X_test, y_train, y_test, best_params):\n",
    "    clf = xgb.XGBClassifier(objective='binary:logistic', seed=11, **best_params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return y_pred, y_pred_proba, clf, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBayesianOptimization(X_train, y_train):\n",
    "    param_space = {\n",
    "        'n_estimators': (50, 200),\n",
    "        'max_depth': (3, 10),\n",
    "        'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "        'subsample': (0.7, 1.0),\n",
    "        'colsample_bytree': (0.7, 1.0)\n",
    "    }\n",
    "    \n",
    "    clf = xgb.XGBClassifier(objective='binary:logistic', seed=41)\n",
    "    \n",
    "    bayes_search = BayesSearchCV(estimator=clf, search_spaces=param_space, scoring='accuracy', cv=5, n_jobs=-1, n_iter=30, random_state=42)\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = bayes_search.best_params_\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runShap(model, X_train, X_test, target_name, dirPath, fraStr):\n",
    "    logging.info(f\"Calculating SHAP values for target: {target_name}\")\n",
    "    logging.info(f\"X_test columns: {X_test.columns}\")\n",
    "    logging.info(f\"X_test indices: {X_test.index}\")\n",
    "    logging.info(f\"X_test shape: {X_test.shape}\")\n",
    "    logging.info(f\"Model: {model}\")\n",
    "    \n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        logging.info(f\"Multi-class model detected. SHAP values list length: {len(shap_values)}\")\n",
    "        num_classes = len(shap_values)\n",
    "    else:\n",
    "        logging.info(f\"Single-class model detected or unexpected SHAP values shape. SHAP values shape: {shap_values.shape}\")\n",
    "        shap_values = [shap_values]\n",
    "        num_classes = 1\n",
    "    \n",
    "    feature_names = X_test.columns.tolist()\n",
    "    \n",
    "    logging.info(f\"Feature names: {feature_names}\")\n",
    "    logging.info(f\"SHAP values shape after selection: {shap_values[0].shape}\")\n",
    "\n",
    "    for class_shap_values in shap_values:\n",
    "        assert class_shap_values.shape[1] == len(feature_names), \"Mismatch between feature names and SHAP values dimensions\"\n",
    "\n",
    "    dirPath = dirPath + 'SHAP/'\n",
    "    os.makedirs(dirPath, exist_ok=True)\n",
    "\n",
    "    feature_names = np.array(feature_names)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        class_name = getStringValue(target_name, class_idx)\n",
    "        file_name = f'[Shap]{target_name}_{class_name}_bar_{fraStr}.png'\n",
    "        \n",
    "        shap.summary_plot(shap_values[class_idx], X_test, plot_type=\"bar\", show=False, feature_names=feature_names)\n",
    "        plt.savefig(os.path.join(dirPath, file_name))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* alternatives: \n",
    "    * confusion matrix: the plotting functions can be moved after data reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotConfusionMatrix(y_test, y_pred, target_name, dirPath, fraStr):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    disp.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)\n",
    "    ax.set_title(f'Confusion Matrix for {target_name}')\n",
    "    \n",
    "    dirPath = dirPath + 'ConfusionMatrix/'\n",
    "    os.makedirs(dirPath, exist_ok=True)\n",
    "    plt.savefig(dirPath + f'[ConfusionMatrix]{target_name}_{fraStr}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotNormalizedConfusionMatrix(y_test, y_pred, target_name, dirPath, fraStr):\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    disp.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)\n",
    "    ax.set_title(f'Normalized Confusion Matrix for {target_name}')\n",
    "    \n",
    "    dirPath = dirPath + 'ConfusionMatrix/'\n",
    "    os.makedirs(dirPath, exist_ok=True)\n",
    "    plt.savefig(dirPath + f'[NormalizedConfusionMatrix]{target_name}_{fraStr}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveClassificationReport(y_test, y_pred, target_name, dirPath, fraStr):\n",
    "    report = classification_report(y_test, y_pred, target_names=[f'Class {i}' for i in np.unique(y_test)])\n",
    "    \n",
    "    dirPath = dirPath + 'Report/'\n",
    "    os.makedirs(dirPath, exist_ok=True)\n",
    "    with open(os.path.join(dirPath, f'[ClassificationReport]{target_name}_{fraStr}.txt'), 'w') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResult(results, target_name, dirPath, fraStr):\n",
    "    os.makedirs(dirPath, exist_ok=True)\n",
    "    file_path = os.path.join(dirPath, f'{target_name}_results_{fraStr}.txt')\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"Results for {key}:\\n\")\n",
    "            f.write(f\"  Cross-validation scores: {value['cross_val_scores']}\\n\")\n",
    "            f.write(f\"  Average accuracy: {value['avg_accuracy']:.4f}\\n\")\n",
    "            f.write(\"  Best parameters:\\n\")\n",
    "            for param, param_value in value['best_params'].items():\n",
    "                f.write(f\"    {param}: {param_value}\\n\")\n",
    "            f.write(f\"  Accuracies: {value['accuracies']}\\n\")\n",
    "            f.write(f\"  Best accuracy: {value['best_accuracy']:.4f}\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveClassifications(X_test, y_test, y_pred, y_pred_proba, target_name, dirPath, fraStr):\n",
    "    os.makedirs(dirPath, exist_ok=True)\n",
    "    classification_df = pd.DataFrame({\n",
    "        'X_test': X_test.index,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    })\n",
    "    classification_df.to_csv(os.path.join(dirPath, f'{target_name}_classification_{fraStr}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* due to heavy computational cost, sample some of the whole data\n",
    "* in case the whole data are used, one iteration takes more than 1450 min for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Sample Size: 870384\n"
     ]
    }
   ],
   "source": [
    "FRACTION = 1.0\n",
    "analysisSampleSize = int(NFLplays.shape[0]*FRACTION)\n",
    "print(f'Analysis Sample Size: {analysisSampleSize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertFractionToString(fraction):\n",
    "    return f\"{fraction:.2f}\".replace('.', '').zfill(3) # with three digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction: 100\n"
     ]
    }
   ],
   "source": [
    "print(f'Fraction: {convertFractionToString(FRACTION)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPlayTypeClassification(df, fraction, n_splits):\n",
    "    dirPath = '../PlayTypeClassification/Classification/'\n",
    "    os.makedirs(dirPath, exist_ok=True)\n",
    "    logging.basicConfig(filename=dirPath + 'classification.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    if fraction == 1.0:\n",
    "        df_sampled = df\n",
    "    else:\n",
    "        analysisSampleSize = int(df.shape[0]*fraction)\n",
    "        df_sampled = df.sample(n=analysisSampleSize, random_state=17)\n",
    "    \n",
    "    X = getCircumstance(df_sampled)\n",
    "    targets = ['playType', 'huddle', 'formation']\n",
    "    \n",
    "    results = {}\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=97)\n",
    "    \n",
    "    for target in tqdm(targets, desc=\"Processing Targets\"):\n",
    "        logging.info(f\"------- Processing {target}... -------\")\n",
    "        y = df_sampled[target]\n",
    "        \n",
    "        model = xgb.XGBClassifier(objective='multi:softprob', seed=43)\n",
    "        scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "        \n",
    "        avg_accuracy = scores.mean()\n",
    "        logging.info(f\"{target} Cross-validation scores: {scores}\")\n",
    "        logging.info(f\"{target} Average cross-validation accuracy: {avg_accuracy}\")\n",
    "        \n",
    "        accuracies = []\n",
    "        best_params = None\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            \n",
    "            best_params = runBayesianOptimization(X_train, y_train)\n",
    "            \n",
    "            y_pred, y_pred_proba, best_clf, accuracy = XGBClassifierCore(X_train, X_test, y_train, y_test, best_params)\n",
    "            \n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "            fraStr = convertFractionToString(fraction)\n",
    "            \n",
    "            # classification specific plots and reports\n",
    "            runShap(best_clf, X_train, X_test, target, dirPath, fraStr)\n",
    "            plotConfusionMatrix(y_test, y_pred, target, dirPath, fraStr)\n",
    "            plotNormalizedConfusionMatrix(y_test, y_pred, target, dirPath, fraStr)\n",
    "            saveClassificationReport(y_test, y_pred, target, dirPath, fraStr)\n",
    "            \n",
    "            if accuracy == max(accuracies):\n",
    "                saveClassifications(X_test, y_test, y_pred, y_pred_proba, target, dirPath, fraStr)\n",
    "        \n",
    "        results[target] = {\n",
    "            'cross_val_scores': scores,\n",
    "            'avg_accuracy': avg_accuracy,\n",
    "            'best_params': best_params,\n",
    "            'accuracies': accuracies,\n",
    "            'best_accuracy': max(accuracies),\n",
    "        }\n",
    "        logging.info(f\"{target} Average Classification Accuracy: {avg_accuracy}\")\n",
    "        logging.info(f\"Best Parameters for {target}: {best_params}\")\n",
    "    \n",
    "        saveResult(results, target, dirPath, fraStr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Targets:  33%|███▎      | 1/3 [3:55:33<7:51:06, 14133.20s/it]c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Processing Targets:  67%|██████▋   | 2/3 [5:10:38<2:21:09, 8469.66s/it] c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yhjo7\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Processing Targets: 100%|██████████| 3/3 [8:05:01<00:00, 9700.40s/it]  \n"
     ]
    }
   ],
   "source": [
    "runPlayTypeClassification(NFLplays, FRACTION, 5)\n",
    "# 1.0 : 485 min\n",
    "# 0.33: 152 min\n",
    "# 0.1 : 42 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I first used .pkl as the result format but later changed to .txt\n",
    "* So there are .pkls that store the result of the first successful classification run with fraction of 0.33\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadResult(directory, target_name, fraction):\n",
    "    fraStr = convertFractionToString(fraction)\n",
    "    file_path = os.path.join(directory, f'{target_name}_results_{fraStr}.txt')\n",
    "    results = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        key = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"Results for \"):\n",
    "                key = line.replace(\"Results for \", \"\").replace(\":\", \"\")\n",
    "                results[key] = {}\n",
    "            elif line.startswith(\"  Cross-validation scores:\"):\n",
    "                results[key]['cross_val_scores'] = eval(line.split(\": \", 1)[1])\n",
    "            elif line.startswith(\"  Average accuracy:\"):\n",
    "                results[key]['avg_accuracy'] = float(line.split(\": \", 1)[1])\n",
    "            elif line.startswith(\"  Best parameters:\"):\n",
    "                results[key]['best_params'] = {}\n",
    "            elif line.startswith(\"    \"):\n",
    "                param, param_value = line.split(\": \", 1)\n",
    "                results[key]['best_params'][param.strip()] = eval(param_value.strip())\n",
    "            elif line.startswith(\"  Accuracies:\"):\n",
    "                results[key]['accuracies'] = eval(line.split(\": \", 1)[1])\n",
    "            elif line.startswith(\"  Best accuracy:\"):\n",
    "                results[key]['best_accuracy'] = float(line.split(\": \", 1)[1])\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_results = loadResult('../PlayTypeClassification/Classification', 'playType', 0.33)\n",
    "# print(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getResult(directory, target_name, fraction):\n",
    "#     fraStr = convertFractionToString(fraction)\n",
    "#     file_path = os.path.join(directory, f'{target_name}_results_{fraStr}.pkl')\n",
    "#     with open(file_path, 'rb') as f:\n",
    "#         results = pickle.load(f)\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'playType': {'cross_val_scores': array([0.80181388, 0.80389938, 0.8039342 , 0.80471756, 0.8048046 ]),\n",
       "  'avg_accuracy': 0.8038339217899647,\n",
       "  'best_params': OrderedDict([('colsample_bytree', 0.7010890921764229),\n",
       "               ('learning_rate', 0.16144853360713746),\n",
       "               ('max_depth', 8),\n",
       "               ('n_estimators', 88),\n",
       "               ('subsample', 0.8735993008685154)]),\n",
       "  'accuracies': [0.8029627824391603,\n",
       "   0.8044390286360867,\n",
       "   0.8045957002350074,\n",
       "   0.8053616502741753,\n",
       "   0.8047697797893637],\n",
       "  'best_accuracy': 0.8053616502741753}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getResult('../PlayTypeClassification/Classification/', 'playType', 0.33)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
